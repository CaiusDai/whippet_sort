#include "benchmark/benchmark.h"

#include <fcntl.h>
#include <array>
#include <cmath>
#include <cstdlib>
#include <limits>
#include <random>
#include <string>

#include "arrow/array.h"
#include "arrow/array/builder_binary.h"
#include "arrow/array/builder_dict.h"
#include "arrow/io/buffered.h"
#include "arrow/io/file.h"
#include "arrow/io/memory.h"
#include "arrow/testing/gtest_util.h"
#include "arrow/testing/random.h"
#include "arrow/testing/util.h"
#include "arrow/type.h"
#include "arrow/util/config.h"
#include "arrow/visit_data_inline.h"

#include "parquet/column_reader.h"
#include "parquet/column_writer.h"
#include "parquet/encoding.h"
#include "parquet/file_reader.h"
#include "parquet/metadata.h"
#include "parquet/platform.h"
#include "parquet/schema.h"
#include "parquet/thrift_internal.h"

#include "parquet/benchmark_util.h"

using arrow::default_memory_pool;
using arrow::MemoryPool;

namespace {
const std::string tmp_file_path = "/tmp/bm_tmp.parquet";
// Maximum range for integer and double data generation
constexpr int MAX_GENERATE_RANGE = 1e9;
// The min/max number of values used to drive each family of encoding benchmarks
constexpr int MIN_SIZE = 4096;
constexpr int MAX_SIZE = 65536;
// Seed value used by random generator
constexpr int SEED = 1337;
// Cardinality Setting, 0 means totally random
constexpr int CARD_NARROW = 10;
constexpr int CARD_MEDIUM = 1000;
constexpr int CARD_WIDE = 0;
}  // namespace

namespace parquet {

using schema::PrimitiveNode;

/************************Data Generators******************************/

/**
 * Given a vector of values, apply cardinality to a specific size
 * @param cardinality The number of unique values in the sequence.
 * @param values The input values. They are assumed to be unique.
 * @param size The size of the sequence to generate.
 */
template <typename T>
static auto ApplyCardinality(int cardinality, std::vector<T> values, int size) {
  if (cardinality == 0 || cardinality >= size) {
    return values;
  }
  auto result = std::vector<T>(size);
  for (int i = 0; i < size; i++) {
    // Potential problem: Reference copy for ByteArray
    result[i] = values[i % cardinality];
  }
  // Shuffle
  std::random_device rd;
  std::shuffle(result.begin(), result.end(), std::default_random_engine(rd()));
  return result;
}

/**
 * Generate a sequence of int64_t values, all with the same value
 * @param data_size The size of the sequence to generate.
 * @param fixed_value The fixed value to generate. Default: 64
 */
std::vector<int64_t> GenerateFixedInt64(size_t data_size, int64_t fixed_value = 64) {
  std::vector<int64_t> data(data_size, fixed_value);
  return data;
}

/**
 * Generate a sequential sequence of int64_t values.
 * The sequence is generated by adding a step to the previous value.
 * @param data_size The size of the sequence to generate.
 * @param start_value The starting value of the sequence.
 * @param shuffle Whether to shuffle the sequence.
 * @param random_step Whether to use a random step. If true, data[i]-data[i-1] will be
 * [step_min, step_max], else 1.
 * @param step_min The minimum step size, used if random_step is true. Default 1.
 * @param step_max The maximum step size, used if random_step is true. Default 10.
 */
std::vector<int64_t> GenerateSeqInt64(size_t data_size, int64_t start_value,
                                      bool shuffle = false, bool random_step = false,
                                      int64_t step_min = 1, int64_t step_max = 10) {
  std::vector<int64_t> data(data_size);
  // Initialize Step Generation, only used if random_step is true
  std::random_device rd;
  std::default_random_engine gen(rd());
  std::uniform_int_distribution<int64_t> d(step_min, step_max);
  data[0] = start_value;
  for (uint32_t i = 1; i < data_size; ++i) {
    if (random_step) {
      data[i] = data[i - 1] + d(gen);
    } else {
      data[i] = data[i - 1] + 1;
    }
  }
  if (shuffle) {
    std::random_device rd;
    std::shuffle(data.begin(), data.end(), std::default_random_engine(rd()));
  }
  return data;
}

/**
 * Generate a sequence of Int64 values with uniform distribution
 * @param data_size The size of the sequence to generate.
 * @param cardinality The number of unique values in the sequence. If 0, all values are
 * unique.
 */
std::vector<int64_t> GenerateUniformInt64(size_t data_size, int cardinality) {
  std::default_random_engine gen(SEED);

  std::uniform_int_distribution<int64_t> d(0, MAX_GENERATE_RANGE);
  if (cardinality == 0) {
    std::vector<int64_t> data(data_size);
    for (uint32_t i = 0; i < data_size; ++i) {
      data[i] = d(gen);
    }
    return data;
  } else {
    std::vector<int64_t> data(cardinality);
    for (uint32_t i = 0; i < static_cast<uint32_t>(cardinality); ++i) {
      data[i] = d(gen);
    }
    return ApplyCardinality(cardinality, data, data_size);
  }
}

/**
 * Generate a sequence of double values with uniform distribution
 * Range: [0, MAX_GENERATE_RANGE)
 * @param data_size The size of the sequence to generate.
 * @param cardinality The number of unique values in the sequence. If 0, all values are
 * unique.
 */
std::vector<double> GenerateUniformDouble(size_t data_size, int cardinality) {
  std::default_random_engine gen(SEED);

  std::uniform_real_distribution<double> d(0, MAX_GENERATE_RANGE);
  if (cardinality == 0) {
    std::vector<double> data(data_size);
    for (uint32_t i = 0; i < data_size; ++i) {
      data[i] = d(gen);
    }
    return data;
  } else {
    std::vector<double> data(cardinality);
    for (uint32_t i = 0; i < static_cast<uint32_t>(cardinality); ++i) {
      data[i] = d(gen);
    }
    return ApplyCardinality(cardinality, data, data_size);
  }
}

/**
 * Generate a sequence of ByteArray values.
 * @param length The size of the sequence to generate.
 * @param cardinality The number of unique values in the sequence. If 0, all values are
 * unique.
 * @param str_min_length The minimum length of the string.
 * @param str_max_length The maximum length of the string.
 * @param byte_processed The number of bytes processed. This field
 * will be modified to correct result after generation. One can use this
 * to assign the expected number of bytes processed in the benchmark.
 *
 */
static auto GenerateByteArrayInput(size_t length, int cardinality, int str_min_length,
                                   int str_max_length, int64_t& byte_processed) {
  ::arrow::random::RandomArrayGenerator rgen(SEED);
  if (cardinality == 0) {
    cardinality = length;
  }

  auto arrow_array = rgen.String(/* size */ cardinality, /* min_length */ str_min_length,
                                 /* max_length */ str_max_length,
                                 /* null_probability */ 0);
  auto byte_array = std::static_pointer_cast<::arrow::StringArray>(arrow_array);
  byte_processed =
      (byte_array->value_data()->size() + byte_array->value_offsets()->size()) *
      (length / cardinality);
  int reminder = length % cardinality;
  if (reminder != 0) {
    auto slice =
        std::static_pointer_cast<::arrow::StringArray>(byte_array->Slice(0, reminder));
    byte_processed += (slice->value_data()->size() + slice->value_offsets()->size());
  }
  std::vector<ByteArray> values;
  for (int i = 0; i < byte_array->length(); ++i) {
    values.emplace_back(byte_array->GetView(i));
  }
  return ApplyCardinality(cardinality, values, length);
}

/************************Helper functions***************************/
// Used by File Writing tests
template <typename WriterType>
std::shared_ptr<WriterType> BuildWriter(int64_t output_size,
                                        const std::shared_ptr<ArrowOutputStream>& dst,
                                        ColumnChunkMetaDataBuilder* metadata,
                                        ColumnDescriptor* schema,
                                        const WriterProperties* properties,
                                        Compression::type codec) {
  std::unique_ptr<PageWriter> pager = PageWriter::Open(dst, codec, metadata);
  std::shared_ptr<ColumnWriter> writer =
      ColumnWriter::Make(metadata, std::move(pager), properties);
  return std::static_pointer_cast<WriterType>(writer);
}

template <typename ReadType>
std::shared_ptr<ReadType> BuildReader(std::shared_ptr<ArrowInputStream> src,
                                      int64_t num_values, Compression::type codec,
                                      ColumnDescriptor* schema) {
  std::unique_ptr<PageReader> page_reader = PageReader::Open(src, num_values, codec);
  return std::static_pointer_cast<ReadType>(
      ColumnReader::Make(schema, std::move(page_reader)));
}

// These Schemas are used in File IO Tests
std::shared_ptr<ColumnDescriptor> Int64Schema(Repetition::type repetition) {
  auto node = PrimitiveNode::Make("int64", repetition, Type::INT64);
  return std::make_shared<ColumnDescriptor>(node, repetition != Repetition::REQUIRED,
                                            repetition == Repetition::REPEATED);
}

std::shared_ptr<ColumnDescriptor> DoubleSchema(Repetition::type repetition) {
  auto node = PrimitiveNode::Make("double", repetition, Type::DOUBLE);
  return std::make_shared<ColumnDescriptor>(node, repetition != Repetition::REQUIRED,
                                            repetition == Repetition::REPEATED);
}

std::shared_ptr<ColumnDescriptor> ByteArraySchema(Repetition::type repetition) {
  auto node = PrimitiveNode::Make("byte_array", repetition, Type::BYTE_ARRAY);
  return std::make_shared<ColumnDescriptor>(node, repetition != Repetition::REQUIRED,
                                            repetition == Repetition::REPEATED);
}

// Helper functions for Dictionary Encoding
template <typename Type>
static void EncodeDict(const std::vector<typename Type::c_type>& values,
                       ::benchmark::State& state,
                       std::shared_ptr<ColumnDescriptor> descr) {
  using T = typename Type::c_type;
  int num_values = static_cast<int>(values.size());

  MemoryPool* allocator = default_memory_pool();

  // Note: Encoding input is not used when use_dictionary is set to true
  auto base_encoder = MakeEncoder(Type::type_num, Encoding::RLE_DICTIONARY,
                                  /*use_dictionary=*/true, descr.get(), allocator);
  auto encoder =
      dynamic_cast<typename EncodingTraits<Type>::Encoder*>(base_encoder.get());
  for (auto _ : state) {
    encoder->Put(values.data(), num_values);
    encoder->FlushValues();
  }

  state.SetBytesProcessed(state.iterations() * num_values * sizeof(T));
  state.SetItemsProcessed(state.iterations() * num_values);
}

template <typename Type>
static void DecodeDict(const std::vector<typename Type::c_type>& values,
                       ::benchmark::State& state,
                       std::shared_ptr<ColumnDescriptor> descr) {
  typedef typename Type::c_type T;
  int num_values = static_cast<int>(values.size());

  MemoryPool* allocator = default_memory_pool();

  auto base_encoder =
      MakeEncoder(Type::type_num, Encoding::PLAIN, true, descr.get(), allocator);
  auto encoder =
      dynamic_cast<typename EncodingTraits<Type>::Encoder*>(base_encoder.get());
  auto dict_traits = dynamic_cast<DictEncoder<Type>*>(base_encoder.get());
  encoder->Put(values.data(), num_values);

  std::shared_ptr<ResizableBuffer> dict_buffer =
      AllocateBuffer(allocator, dict_traits->dict_encoded_size());

  std::shared_ptr<ResizableBuffer> indices =
      AllocateBuffer(allocator, encoder->EstimatedDataEncodedSize());

  dict_traits->WriteDict(dict_buffer->mutable_data());
  int actual_bytes = dict_traits->WriteIndices(indices->mutable_data(),
                                               static_cast<int>(indices->size()));

  PARQUET_THROW_NOT_OK(indices->Resize(actual_bytes));

  std::vector<T> decoded_values(num_values);
  for (auto _ : state) {
    auto dict_decoder = MakeTypedDecoder<Type>(Encoding::PLAIN, descr.get());
    dict_decoder->SetData(dict_traits->num_entries(), dict_buffer->data(),
                          static_cast<int>(dict_buffer->size()));

    auto decoder = MakeDictDecoder<Type>(descr.get());
    decoder->SetDict(dict_decoder.get());
    decoder->SetData(num_values, indices->data(), static_cast<int>(indices->size()));
    decoder->Decode(decoded_values.data(), num_values);
  }

  state.SetBytesProcessed(state.iterations() * num_values * sizeof(T));
  state.SetItemsProcessed(state.iterations() * num_values);
}

template <typename Type, typename NumberGenerator>
static void BM_DictEncoding(::benchmark::State& state, NumberGenerator gen,
                            int cardinality, std::shared_ptr<ColumnDescriptor> descr) {
  using T = typename Type::c_type;
  std::vector<T> values = gen(state.range(0) /*Size*/, cardinality);
  EncodeDict<Type>(values, state, descr);
}

template <typename WriterType, typename Type>
static void BM_WriteColumn(::benchmark::State& state, Compression::type codec,
                           Encoding::type encoding, bool use_dict,
                           std::vector<typename Type::c_type> input_values,
                           std::shared_ptr<parquet::ColumnDescriptor> schema,
                           int64_t bytes_processed = 0) {
  format::ColumnChunk thrift_metadata;
  using T = typename Type::c_type;
  int num_values = static_cast<int>(input_values.size());
  std::vector<int16_t> definition_levels(num_values, 1);
  std::vector<int16_t> repetition_levels(num_values, 0);
  std::shared_ptr<WriterProperties> properties;
  if (use_dict) {
    properties =
        WriterProperties::Builder().compression(codec)->encoding(encoding)->build();
  } else {
    properties = WriterProperties::Builder()
                     .compression(codec)
                     ->encoding(encoding)
                     ->disable_dictionary()
                     ->build();
  }
  auto metadata = ColumnChunkMetaDataBuilder::Make(
      properties, schema.get(), reinterpret_cast<uint8_t*>(&thrift_metadata));
  int64_t stream_size = 0;  // Real number of bytes written to the stream.
  for (auto _ : state) {
    // Clear the filesystem cache (requires root access)
    //     auto code = system("echo 1 | sudo tee /proc/sys/vm/drop_caches > /dev/null");

    auto drop_cache = system(
        "dd of=/tmp/bm_tmp.parquet oflag=nocache conv=notrunc,fdatasync status=none "
        "count=0");
    // Create a file output stream
    std::shared_ptr<::arrow::io::FileOutputStream> stream;
    PARQUET_ASSIGN_OR_THROW(stream, ::arrow::io::FileOutputStream::Open(
                                        tmp_file_path, false /*ie.: Truncate*/));

    // Start timing
    auto start = std::chrono::high_resolution_clock::now();
    std::shared_ptr<WriterType> writer = BuildWriter<WriterType>(
        num_values, stream, metadata.get(), schema.get(), properties.get(), codec);
    writer->WriteBatch(input_values.size(), definition_levels.data(),
                       repetition_levels.data(), input_values.data());
    writer->Close();
    auto end = std::chrono::high_resolution_clock::now();
    auto elapsed_seconds =
        std::chrono::duration_cast<std::chrono::duration<double>>(end - start);
    stream_size = stream->Tell().ValueOrDie();
    state.SetIterationTime(elapsed_seconds.count());
  }
  // When user gives expected bytes to process for each iteration, use it.
  if (bytes_processed == 0) {
    state.SetBytesProcessed(state.iterations() * num_values * sizeof(T));
    state.counters["compression_ratio"] =
        stream_size * 100 / static_cast<double>(num_values * sizeof(T));
  } else {
    state.SetBytesProcessed(state.iterations() * bytes_processed);
    state.counters["compression_ratio"] =
        stream_size * 100 / static_cast<double>(bytes_processed);
  }
  state.SetItemsProcessed(state.iterations() * num_values);
  state.counters["written_size"] = stream_size;
}

template <typename WriterType, typename ReaderType, typename DType>
static void BM_ReadColumn(::benchmark::State& state, Compression::type codec,
                          Encoding::type encoding, bool use_dict,
                          std::vector<typename DType::c_type> input_values,
                          std::shared_ptr<parquet::ColumnDescriptor> schema,
                          int64_t bytes_processed = 0) {
  using T = typename DType::c_type;
  format::ColumnChunk thrift_metadata;
  int num_values = static_cast<int>(input_values.size());
  std::vector<int16_t> definition_levels(num_values, 1);
  std::vector<int16_t> repetition_levels(num_values, 0);
  std::shared_ptr<WriterProperties> properties;
  if (use_dict) {
    // Dictionary is enabled by default, thus need special handling
    properties =
        WriterProperties::Builder().compression(codec)->encoding(encoding)->build();
  } else {
    properties = WriterProperties::Builder()
                     .compression(codec)
                     ->encoding(encoding)
                     ->disable_dictionary()
                     ->build();
  }

  auto metadata = ColumnChunkMetaDataBuilder::Make(
      properties, schema.get(), reinterpret_cast<uint8_t*>(&thrift_metadata));

  std::shared_ptr<::arrow::io::FileOutputStream> stream;
  PARQUET_ASSIGN_OR_THROW(stream, ::arrow::io::FileOutputStream::Open(
                                      tmp_file_path, false /*i.e.: Truncate*/));
  std::shared_ptr<WriterType> writer = BuildWriter<WriterType>(
      num_values, stream, metadata.get(), schema.get(), properties.get(), codec);
  writer->WriteBatch(input_values.size(), definition_levels.data(),
                     repetition_levels.data(), input_values.data());
  writer->Close();
  auto written_size = stream->Tell().ValueOrDie();
  // state.range(1): Read size
  std::vector<T> values_out(num_values);
  std::vector<int16_t> definition_levels_out(num_values);
  std::vector<int16_t> repetition_levels_out(num_values);
  for (auto _ : state) {
    // Drop cache
    auto drop_cache = system(
        "dd of=/tmp/bm_tmp.parquet oflag=nocache conv=notrunc,fdatasync status=none "
        "count=0");
    // Create a file input stream
    std::shared_ptr<::arrow::io::RandomAccessFile> raw_src_file;
    PARQUET_ASSIGN_OR_THROW(raw_src_file, ::arrow::io::ReadableFile::Open(tmp_file_path));
    // Determine the size of the file
    int64_t file_size = 0;
    PARQUET_ASSIGN_OR_THROW(file_size, raw_src_file->GetSize());
    if (written_size != file_size) {
      std::cout << "Written Size: " << written_size << std::endl;
      std::cout << "File Size: " << file_size << std::endl;
      throw std::runtime_error("File size mismatch");
    }
    std::shared_ptr<::arrow::Buffer> buffer;
    // Start Timing
    auto start = std::chrono::high_resolution_clock::now();
    // Read data in memory
    PARQUET_ASSIGN_OR_THROW(buffer, raw_src_file->Read(file_size));
    // Check the reading size
    if (file_size != buffer->size()) {
      std::cout << "File Size: " << file_size << std::endl;
      std::cout << "Buffer Size: " << buffer->size() << std::endl;
      throw std::runtime_error("File size mismatch");
    }
    auto buffer_reader = std::make_shared<::arrow::io::BufferReader>(buffer);
    std::shared_ptr<ReaderType> reader =
        BuildReader<ReaderType>(buffer_reader, num_values, codec, schema.get());
    int64_t values_read = 0;
    for (size_t i = 0; i < input_values.size(); i += values_read) {
      reader->ReadBatch(values_out.size(), definition_levels_out.data(),
                        repetition_levels_out.data(), values_out.data(), &values_read);
    }
    auto end = std::chrono::high_resolution_clock::now();
    auto elapsed_seconds =
        std::chrono::duration_cast<std::chrono::duration<double>>(end - start);
    auto read_size = buffer->size();
    if (read_size != written_size) {
      std::cout << "Read Size: " << read_size << std::endl;
      std::cout << "Written Size: " << written_size << std::endl;
      throw std::runtime_error("Read size mismatch");
    }
    state.SetIterationTime(elapsed_seconds.count());

    // Release resources
    raw_src_file->Close();
    buffer_reader->Close();
  }
  // When user gives expected bytes to process for each iteration, use it.
  if (bytes_processed == 0) {
    state.SetBytesProcessed(state.iterations() * num_values * sizeof(T));
  } else {
    state.SetBytesProcessed(state.iterations() * bytes_processed);
  }
  state.SetItemsProcessed(state.iterations() * num_values);
}

/*******************Integer  Plain Tests*******************/
// Int64 Plain Encode Test
// NumberGenerator: Function that can produce input values
template <typename NumberGenerator>
static void BM_PlainEncodingInt64(::benchmark::State& state, NumberGenerator gen,
                                  int cardinality) {
  std::vector<int64_t> values = gen(state.range(0) /*Size*/, cardinality);
  auto encoder = MakeTypedEncoder<Int64Type>(Encoding::PLAIN);
  for (auto _ : state) {
    encoder->Put(values.data(), static_cast<int>(values.size()));
    encoder->FlushValues();
  }
  state.SetBytesProcessed(state.iterations() * state.range(0) * sizeof(int64_t));
  state.SetItemsProcessed(state.iterations() * values.size());
}

static void BM_PlainEncodingInt64_Narrow(::benchmark::State& state) {
  BM_PlainEncodingInt64(state, GenerateUniformInt64, CARD_NARROW);
}

static void BM_PlainEncodingInt64_Medium(::benchmark::State& state) {
  BM_PlainEncodingInt64(state, GenerateUniformInt64, CARD_MEDIUM);
}

static void BM_PlainEncodingInt64_Wide(::benchmark::State& state) {
  BM_PlainEncodingInt64(state, GenerateUniformInt64, CARD_WIDE);
}

// To test the performance for different data sizes, use Range() instead of Args()
// eg: BENCHMARK(...)->Range(MIN_SIZE, MAX_SIZE);
BENCHMARK(BM_PlainEncodingInt64_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_PlainEncodingInt64_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_PlainEncodingInt64_Wide)->Args({MAX_SIZE});

// Int64 Plain Decoding Test
template <typename NumberGenerator>
static void BM_PlainDecodingInt64(::benchmark::State& state, NumberGenerator gen,
                                  int cardinality) {
  std::vector<int64_t> values = gen(state.range(0), cardinality);
  auto encoder = MakeTypedEncoder<Int64Type>(Encoding::PLAIN);
  encoder->Put(values.data(), static_cast<int>(values.size()));
  std::shared_ptr<Buffer> buf = encoder->FlushValues();
  auto decoder = MakeTypedDecoder<Int64Type>(Encoding::PLAIN);
  for (auto _ : state) {
    decoder->SetData(static_cast<int>(values.size()), buf->data(),
                     static_cast<int>(buf->size()));
    decoder->Decode(values.data(), static_cast<int>(values.size()));
  }
  state.SetBytesProcessed(state.iterations() * state.range(0) * sizeof(int64_t));
  state.SetItemsProcessed(state.iterations() * values.size());
}

static void BM_PlainDecodingInt64_Narrow(::benchmark::State& state) {
  BM_PlainDecodingInt64(state, GenerateUniformInt64, CARD_NARROW);
}

static void BM_PlainDecodingInt64_Medium(::benchmark::State& state) {
  BM_PlainDecodingInt64(state, GenerateUniformInt64, CARD_MEDIUM);
}

static void BM_PlainDecodingInt64_Wide(::benchmark::State& state) {
  BM_PlainDecodingInt64(state, GenerateUniformInt64, CARD_WIDE);
}

// To test the performance for different data sizes, use Range() instead of Args()
BENCHMARK(BM_PlainDecodingInt64_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_PlainDecodingInt64_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_PlainDecodingInt64_Wide)->Args({MAX_SIZE});

// Int64 Plain Write Test
template <const int cardinality, Encoding::type encoding, bool use_dict>
static void BM_WriteInt64Column_Plain(::benchmark::State& state) {
  auto input_values = GenerateUniformInt64(state.range(0), cardinality);
  BM_WriteColumn<Int64Writer, Int64Type>(state, Compression::UNCOMPRESSED, encoding,
                                         use_dict, input_values,
                                         Int64Schema(Repetition::REQUIRED));
}

// Int64 Plain  Write Test
BENCHMARK_TEMPLATE(BM_WriteInt64Column_Plain, CARD_NARROW, Encoding::PLAIN, false)
    ->Arg(MAX_SIZE)
    ->UseManualTime();
BENCHMARK_TEMPLATE(BM_WriteInt64Column_Plain, CARD_MEDIUM, Encoding::PLAIN, false)
    ->Arg(MAX_SIZE)
    ->UseManualTime();
BENCHMARK_TEMPLATE(BM_WriteInt64Column_Plain, CARD_WIDE, Encoding::PLAIN, false)
    ->Arg(MAX_SIZE)
    ->UseManualTime();

// Int64 Plain Read test
static void BM_ReadInt64Column_Plain(::benchmark::State& state) {
  auto input_values =
      GenerateUniformInt64(state.range(0) /*Size*/, CARD_MEDIUM /*Cardinality*/);
  BM_ReadColumn<Int64Writer, Int64Reader, Int64Type>(state, Compression::UNCOMPRESSED,
                                                     Encoding::PLAIN, false, input_values,
                                                     Int64Schema(Repetition::REQUIRED));
}

BENCHMARK(BM_ReadInt64Column_Plain)->Args({MAX_SIZE})->UseManualTime();

/****************Int64 DeltaBitPacking Tests************************/
// Int64 DeltaBitPacking Encode Test
static void BM_DeltaBitPackingEncode(::benchmark::State& state,
                                     std::vector<int64_t> values) {
  auto encoder = MakeTypedEncoder<Int64Type>(Encoding::DELTA_BINARY_PACKED);
  for (auto _ : state) {
    encoder->Put(values.data(), static_cast<int>(values.size()));
    encoder->FlushValues();
  }
  state.SetBytesProcessed(state.iterations() * values.size() * sizeof(int64_t));
  state.SetItemsProcessed(state.iterations() * values.size());
}

// All data is the same
static void BM_DeltaBitPackingEncodingInt64_Fixed(::benchmark::State& state) {
  auto values = GenerateFixedInt64(state.range(0) /*Data Size*/);
  BM_DeltaBitPackingEncode(state, values);
}

// Sequential data, with random step size [1,5]
static void BM_DeltaBitPackingEncodingInt64_Sequential(::benchmark::State& state) {
  auto values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                 false /*Shuffle*/, true /*Use Random Step*/,
                                 1 /*Min Step Size*/, 5 /*Max Step Size*/);
  BM_DeltaBitPackingEncode(state, values);
}

// Very scattered data
static void BM_DeltaBitPackingEncodingInt64_Scatter(::benchmark::State& state) {
  auto values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                 true /*Shuffle*/, true /*Use Random Step*/,
                                 1 /*Min Step Size*/, 1000 /*Max Step Size*/);
  BM_DeltaBitPackingEncode(state, values);
}

// To test the performance for different data sizes, use Range() instead of Args()
BENCHMARK(BM_DeltaBitPackingEncodingInt64_Fixed)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaBitPackingEncodingInt64_Sequential)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaBitPackingEncodingInt64_Scatter)->Args({MAX_SIZE});

// Int64 DeltaBitPacking Decode Test
static void BM_DeltaBitPackingDecode(::benchmark::State& state,
                                     std::vector<int64_t> values) {
  auto encoder = MakeTypedEncoder<Int64Type>(Encoding::DELTA_BINARY_PACKED);
  encoder->Put(values.data(), static_cast<int>(values.size()));
  std::shared_ptr<Buffer> buf = encoder->FlushValues();

  auto decoder = MakeTypedDecoder<Int64Type>(Encoding::DELTA_BINARY_PACKED);
  for (auto _ : state) {
    decoder->SetData(static_cast<int>(values.size()), buf->data(),
                     static_cast<int>(buf->size()));
    decoder->Decode(values.data(), static_cast<int>(values.size()));
  }
  state.SetBytesProcessed(state.iterations() * state.range(0) * sizeof(int64_t));
  state.SetItemsProcessed(state.iterations() * state.range(0));
}

// All data is the same
static void BM_DeltaBitPackingDecodingInt64_Fixed(::benchmark::State& state) {
  auto values = GenerateFixedInt64(state.range(0) /*Data Size*/);
  BM_DeltaBitPackingDecode(state, values);
}

// Sequential data
static void BM_DeltaBitPackingDecodingInt64_Sequential(::benchmark::State& state) {
  auto values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                 false /*Shuffle*/, true /*Use Random Step*/,
                                 1 /*Min Step Size*/, 5 /*Max Step Size*/);
  BM_DeltaBitPackingDecode(state, values);
}

// Very scattered data
static void BM_DeltaBitPackingDecodingInt64_Scatter(::benchmark::State& state) {
  auto values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                 true /*Shuffle*/, true /*Use Random Step*/,
                                 1 /*Min Step Size*/, 1000 /*Max Step Size*/);
  BM_DeltaBitPackingDecode(state, values);
}

// To test the performance for different data sizes, use Range() instead of Args()
BENCHMARK(BM_DeltaBitPackingDecodingInt64_Fixed)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaBitPackingDecodingInt64_Sequential)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaBitPackingDecodingInt64_Scatter)->Args({MAX_SIZE});

// Int64 DeltaBitPacking Writing Tests
static void BM_WriteInt64Column_DeltaBitPacking_Fix(::benchmark::State& state) {
  auto input_values = GenerateFixedInt64(state.range(0) /*Data Size*/);
  BM_WriteColumn<Int64Writer, Int64Type>(state, Compression::UNCOMPRESSED,
                                         Encoding::DELTA_BINARY_PACKED, false,
                                         input_values, Int64Schema(Repetition::REQUIRED));
}

static void BM_WriteInt64Column_DeltaBitPacking_Seq(::benchmark::State& state) {
  auto input_values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                       false /*Shuffle*/, true /*Use Random Step*/,
                                       1 /*Min Step Size*/, 5 /*Max Step Size*/);
  BM_WriteColumn<Int64Writer, Int64Type>(state, Compression::UNCOMPRESSED,
                                         Encoding::DELTA_BINARY_PACKED, false,
                                         input_values, Int64Schema(Repetition::REQUIRED));
}

static void BM_WriteInt64Column_DeltaBitPacking_Scatter(::benchmark::State& state) {
  auto input_values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                       true /*Shuffle*/, true /*Use Random Step*/,
                                       1 /*Min Step Size*/, 1000 /*Max Step Size*/);
  BM_WriteColumn<Int64Writer, Int64Type>(state, Compression::UNCOMPRESSED,
                                         Encoding::DELTA_BINARY_PACKED, false,
                                         input_values, Int64Schema(Repetition::REQUIRED));
}

BENCHMARK(BM_WriteInt64Column_DeltaBitPacking_Fix)->Arg(MAX_SIZE)->UseManualTime();
BENCHMARK(BM_WriteInt64Column_DeltaBitPacking_Seq)->Arg(MAX_SIZE)->UseManualTime();
BENCHMARK(BM_WriteInt64Column_DeltaBitPacking_Scatter)->Arg(MAX_SIZE)->UseManualTime();

template <const int cardinality, Encoding::type encoding, bool use_dict>
static void BM_ReadInt64Column(::benchmark::State& state) {
  auto input_values = GenerateUniformInt64(state.range(0), cardinality);
  BM_ReadColumn<Int64Writer, Int64Reader, Int64Type>(state, Compression::UNCOMPRESSED,
                                                     encoding, use_dict, input_values,
                                                     Int64Schema(Repetition::REQUIRED));
}

// Int64 DeltaBitPacking Reading Test
static void BM_ReadInt64Column_DeltaBitPacking_Fix(::benchmark::State& state) {
  auto input_values = GenerateFixedInt64(state.range(0) /*Data Size*/);
  BM_ReadColumn<Int64Writer, Int64Reader, Int64Type>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_BINARY_PACKED, false,
      input_values, Int64Schema(Repetition::REQUIRED));
}

static void BM_ReadInt64Column_DeltaBitPacking_Seq(::benchmark::State& state) {
  auto input_values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                       false /*Shuffle*/, true /*Use Random Step*/,
                                       1 /*Min Step Size*/, 5 /*Max Step Size*/);
  BM_ReadColumn<Int64Writer, Int64Reader, Int64Type>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_BINARY_PACKED, false,
      input_values, Int64Schema(Repetition::REQUIRED));
}

static void BM_ReadInt64Column_DeltaBitPacking_Scatter(::benchmark::State& state) {
  auto input_values = GenerateSeqInt64(state.range(0) /*Data Size*/, 0, /*Starting Value*/
                                       true /*Shuffle*/, true /*Use Random Step*/,
                                       1 /*Min Step Size*/, 1000 /*Max Step Size*/);
  BM_ReadColumn<Int64Writer, Int64Reader, Int64Type>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_BINARY_PACKED, false,
      input_values, Int64Schema(Repetition::REQUIRED));
}

BENCHMARK(BM_ReadInt64Column_DeltaBitPacking_Fix)->Arg(MAX_SIZE)->UseManualTime();
BENCHMARK(BM_ReadInt64Column_DeltaBitPacking_Seq)->Arg(MAX_SIZE)->UseManualTime();
BENCHMARK(BM_ReadInt64Column_DeltaBitPacking_Scatter)->Arg(MAX_SIZE)->UseManualTime();

/****************Int64 Dictionary Test************************/
// Int64 Dictionary Encode Test
static void BM_DictEncodingInt64_Narrow(::benchmark::State& state) {
  BM_DictEncoding<Int64Type>(state, GenerateUniformInt64, CARD_NARROW,
                             Int64Schema(Repetition::REQUIRED));
}

static void BM_DictEncodingInt64_Medium(::benchmark::State& state) {
  BM_DictEncoding<Int64Type>(state, GenerateUniformInt64, CARD_MEDIUM,
                             Int64Schema(Repetition::REQUIRED));
}

static void BM_DictEncodingInt64_Wide(::benchmark::State& state) {
  BM_DictEncoding<Int64Type>(state, GenerateUniformInt64, CARD_WIDE,
                             Int64Schema(Repetition::REQUIRED));
}

// To test the performance for different data sizes, use Range() instead of Args()
BENCHMARK(BM_DictEncodingInt64_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_DictEncodingInt64_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_DictEncodingInt64_Wide)->Args({MAX_SIZE});

// Int64 Dictionary Decode Test
template <typename Type, typename NumberGenerator>
static void BM_DictDecoding(::benchmark::State& state, NumberGenerator gen,
                            int cardinality, std::shared_ptr<ColumnDescriptor> descr) {
  using T = typename Type::c_type;
  std::vector<T> values = gen(state.range(0) /*Size*/, cardinality);
  DecodeDict<Type>(values, state, descr);
}

static void BM_DictDecodingInt64_Narrow(::benchmark::State& state) {
  BM_DictDecoding<Int64Type>(state, GenerateUniformInt64, CARD_NARROW,
                             Int64Schema(Repetition::REQUIRED));
}

static void BM_DictDecodingInt64_Medium(::benchmark::State& state) {
  BM_DictDecoding<Int64Type>(state, GenerateUniformInt64, CARD_MEDIUM,
                             Int64Schema(Repetition::REQUIRED));
}

static void BM_DictDecodingInt64_Wide(::benchmark::State& state) {
  BM_DictDecoding<Int64Type>(state, GenerateUniformInt64, CARD_WIDE,
                             Int64Schema(Repetition::REQUIRED));
}

BENCHMARK(BM_DictDecodingInt64_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_DictDecodingInt64_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_DictDecodingInt64_Wide)->Args({MAX_SIZE});

// Int64 Dictionary Write Tests
static void BM_WriteInt64Column_Dictionary(::benchmark::State& state) {
  int cardinality = state.range(1);
  auto input_values = GenerateUniformInt64(state.range(0), cardinality);
  BM_WriteColumn<Int64Writer, Int64Type>(state, Compression::UNCOMPRESSED,
                                         Encoding::PLAIN, true, input_values,
                                         Int64Schema(Repetition::REQUIRED));
}
BENCHMARK(BM_WriteInt64Column_Dictionary)->Args({MAX_SIZE, CARD_NARROW})->UseManualTime();
BENCHMARK(BM_WriteInt64Column_Dictionary)->Args({MAX_SIZE, CARD_MEDIUM})->UseManualTime();
BENCHMARK(BM_WriteInt64Column_Dictionary)->Args({MAX_SIZE, CARD_WIDE})->UseManualTime();

// Int64 Dictionary Read Test
static void BM_ReadInt64Column_Dictionary(::benchmark::State& state) {
  int cardinality = state.range(1);
  auto input_values = GenerateUniformInt64(state.range(0), cardinality);
  BM_ReadColumn<Int64Writer, Int64Reader, Int64Type>(state, Compression::UNCOMPRESSED,
                                                     Encoding::PLAIN, true, input_values,
                                                     Int64Schema(Repetition::REQUIRED));
}

BENCHMARK(BM_ReadInt64Column_Dictionary)->Args({MAX_SIZE, CARD_NARROW})->UseManualTime();
BENCHMARK(BM_ReadInt64Column_Dictionary)->Args({MAX_SIZE, CARD_MEDIUM})->UseManualTime();
BENCHMARK(BM_ReadInt64Column_Dictionary)->Args({MAX_SIZE, CARD_WIDE})->UseManualTime();

/*********************************Double Plain Test******************************/
static void BM_PlainEncodingDouble(::benchmark::State& state, int cardinality) {
  std::vector<double> values =
      GenerateUniformDouble(state.range(0) /*Size*/, cardinality);
  auto encoder = MakeTypedEncoder<DoubleType>(Encoding::PLAIN);
  for (auto _ : state) {
    encoder->Put(values.data(), static_cast<int>(values.size()));
    encoder->FlushValues();
  }
  state.SetBytesProcessed(state.iterations() * state.range(0) * sizeof(double));
  state.SetItemsProcessed(state.iterations() * state.range(0));
}

static void BM_PlainEncodingDouble_Narrow(::benchmark::State& state) {
  BM_PlainEncodingDouble(state, CARD_NARROW);
}

static void BM_PlainEncodingDouble_Medium(::benchmark::State& state) {
  BM_PlainEncodingDouble(state, CARD_MEDIUM);
}

static void BM_PlainEncodingDouble_Wide(::benchmark::State& state) {
  BM_PlainEncodingDouble(state, CARD_WIDE);
}

// To test the performance for different data sizes, use Range() instead of Args()
BENCHMARK(BM_PlainEncodingDouble_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_PlainEncodingDouble_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_PlainEncodingDouble_Wide)->Args({MAX_SIZE});

// Double Plain Decode Test
static void BM_PlainDecodingDouble(::benchmark::State& state, int cardinality) {
  std::vector<double> values =
      GenerateUniformDouble(state.range(0) /*Size*/, cardinality);
  auto encoder = MakeTypedEncoder<DoubleType>(Encoding::PLAIN);
  encoder->Put(values.data(), static_cast<int>(values.size()));
  std::shared_ptr<Buffer> buf = encoder->FlushValues();

  for (auto _ : state) {
    auto decoder = MakeTypedDecoder<DoubleType>(Encoding::PLAIN);
    decoder->SetData(static_cast<int>(values.size()), buf->data(),
                     static_cast<int>(buf->size()));
    decoder->Decode(values.data(), static_cast<int>(values.size()));
  }
  state.SetBytesProcessed(state.iterations() * state.range(0) * sizeof(double));
  state.SetItemsProcessed(state.iterations() * state.range(0));
}

static void BM_PlainDecodingDouble_Narrow(::benchmark::State& state) {
  BM_PlainDecodingDouble(state, CARD_NARROW);
}

static void BM_PlainDecodingDouble_Medium(::benchmark::State& state) {
  BM_PlainDecodingDouble(state, CARD_MEDIUM);
}

static void BM_PlainDecodingDouble_Wide(::benchmark::State& state) {
  BM_PlainDecodingDouble(state, CARD_WIDE);
}

BENCHMARK(BM_PlainDecodingDouble_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_PlainDecodingDouble_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_PlainDecodingDouble_Wide)->Args({MAX_SIZE});

static void BM_WriteDoubleColumn(::benchmark::State& state, int cardinality,
                                 Encoding::type encoding, bool use_dict = false) {
  auto data_size = state.range(0);
  auto input_values = GenerateUniformDouble(state.range(0), cardinality);
  BM_WriteColumn<DoubleWriter, DoubleType>(state, Compression::UNCOMPRESSED, encoding,
                                           use_dict, input_values,
                                           DoubleSchema(Repetition::REQUIRED));
}

static void BM_WriteDoubleColumn_Plain_Narrow(::benchmark::State& state) {
  BM_WriteDoubleColumn(state, CARD_NARROW, Encoding::PLAIN);
}

static void BM_WriteDoubleColumn_Plain_Medium(::benchmark::State& state) {
  BM_WriteDoubleColumn(state, CARD_MEDIUM, Encoding::PLAIN);
}

static void BM_WriteDoubleColumn_Plain_Wide(::benchmark::State& state) {
  BM_WriteDoubleColumn(state, CARD_WIDE, Encoding::PLAIN);
}

// Double Plain Write Test
BENCHMARK(BM_WriteDoubleColumn_Plain_Narrow)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_WriteDoubleColumn_Plain_Medium)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_WriteDoubleColumn_Plain_Wide)->Args({MAX_SIZE})->UseManualTime();

static void BM_ReadDoubleColumn_Plain(::benchmark::State& state, int cardinality,
                                      Encoding::type encoding, bool use_dict = false) {
  auto input_values = GenerateUniformDouble(state.range(0), cardinality);
  BM_ReadColumn<DoubleWriter, DoubleReader, DoubleType>(
      state, Compression::UNCOMPRESSED, encoding, false, input_values,
      DoubleSchema(Repetition::REQUIRED));
}

static void BM_ReadDoubleColumn_Plain_Narrow(::benchmark::State& state) {
  BM_ReadDoubleColumn_Plain(state, CARD_NARROW, Encoding::PLAIN);
}

static void BM_ReadDoubleColumn_Plain_Medium(::benchmark::State& state) {
  BM_ReadDoubleColumn_Plain(state, CARD_MEDIUM, Encoding::PLAIN);
}

static void BM_ReadDoubleColumn_Plain_Wide(::benchmark::State& state) {
  BM_ReadDoubleColumn_Plain(state, CARD_WIDE, Encoding::PLAIN);
}

// Double Plain Read Test
BENCHMARK(BM_ReadDoubleColumn_Plain_Narrow)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_ReadDoubleColumn_Plain_Medium)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_ReadDoubleColumn_Plain_Wide)->Args({MAX_SIZE})->UseManualTime();

/*******************Double ByteStreamSplit Tests***********************/
// Double Stream Split Encode Test
// Note: In original encoding_benchmark.cc, the benchmark is conducted by
// different encoding functions, but in real parquet implementation,
// only one function is used to encode different encodings. Thus we
// only benchmark one function here.
static void BM_ByteStreamSplitEncode(::benchmark::State& state, int cardinality) {
  std::vector<double> values =
      GenerateUniformDouble(state.range(0) /*Size*/, cardinality);
  auto encoder = MakeTypedEncoder<DoubleType>(Encoding::BYTE_STREAM_SPLIT);
  for (auto _ : state) {
    encoder->Put(values.data(), static_cast<int>(values.size()));
    encoder->FlushValues();
  }
  state.SetBytesProcessed(state.iterations() * values.size() * sizeof(double));
  state.SetItemsProcessed(state.iterations() * values.size());
}

static void BM_ByteStreamSplitEncodeDouble_Narrow(::benchmark::State& state) {
  BM_ByteStreamSplitEncode(state, CARD_NARROW);
}

static void BM_ByteStreamSplitEncodeDouble_Medium(::benchmark::State& state) {
  BM_ByteStreamSplitEncode(state, CARD_MEDIUM);
}

static void BM_ByteStreamSplitEncodeDouble_Wide(::benchmark::State& state) {
  BM_ByteStreamSplitEncode(state, CARD_WIDE);
}

BENCHMARK(BM_ByteStreamSplitEncodeDouble_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_ByteStreamSplitEncodeDouble_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_ByteStreamSplitEncodeDouble_Wide)->Args({MAX_SIZE});

// Double Split Decode Test
static void BM_ByteStreamSplitDecode(::benchmark::State& state, int cardinality) {
  std::vector<double> values =
      GenerateUniformDouble(state.range(0) /*Size*/, cardinality);
  auto encoder = MakeTypedEncoder<DoubleType>(Encoding::BYTE_STREAM_SPLIT);
  encoder->Put(values.data(), static_cast<int>(values.size()));
  std::shared_ptr<Buffer> buf = encoder->FlushValues();
  for (auto _ : state) {
    auto decoder = MakeTypedDecoder<DoubleType>(Encoding::BYTE_STREAM_SPLIT);
    decoder->SetData(static_cast<int>(values.size()), buf->data(),
                     static_cast<int>(buf->size()));
    decoder->Decode(values.data(), static_cast<int>(values.size()));
  }
  state.SetBytesProcessed(state.iterations() * values.size() * sizeof(double));
  state.SetItemsProcessed(state.iterations() * values.size());
}

static void BM_ByteStreamSplitDecodeDouble_Narrow(::benchmark::State& state) {
  BM_ByteStreamSplitDecode(state, CARD_NARROW);
}

static void BM_ByteStreamSplitDecodeDouble_Medium(::benchmark::State& state) {
  BM_ByteStreamSplitDecode(state, CARD_MEDIUM);
}

static void BM_ByteStreamSplitDecodeDouble_Wide(::benchmark::State& state) {
  BM_ByteStreamSplitDecode(state, CARD_WIDE);
}

BENCHMARK(BM_ByteStreamSplitDecodeDouble_Narrow)->Args({MAX_SIZE});
BENCHMARK(BM_ByteStreamSplitDecodeDouble_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_ByteStreamSplitDecodeDouble_Wide)->Args({MAX_SIZE});

// ByteStreamSplit Writing Benchmarks
static void BM_WriteDoubleColumn_ByteStreamSplit_Narrow(::benchmark::State& state) {
  BM_WriteDoubleColumn(state, CARD_NARROW, Encoding::BYTE_STREAM_SPLIT);
}

static void BM_WriteDoubleColumn_ByteStreamSplit_Medium(::benchmark::State& state) {
  BM_WriteDoubleColumn(state, CARD_MEDIUM, Encoding::BYTE_STREAM_SPLIT);
}

static void BM_WriteDoubleColumn_ByteStreamSplit_Wide(::benchmark::State& state) {
  BM_WriteDoubleColumn(state, CARD_WIDE, Encoding::BYTE_STREAM_SPLIT);
}

BENCHMARK(BM_WriteDoubleColumn_ByteStreamSplit_Narrow)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_WriteDoubleColumn_ByteStreamSplit_Medium)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_WriteDoubleColumn_ByteStreamSplit_Wide)->Args({MAX_SIZE})->UseManualTime();

// ByteStreamSplit Reading Benchmarks
static void BM_ReadDoubleColumn_ByteStreamSplit_Narrow(::benchmark::State& state) {
  BM_ReadDoubleColumn_Plain(state, CARD_NARROW, Encoding::BYTE_STREAM_SPLIT);
}

static void BM_ReadDoubleColumn_ByteStreamSplit_Medium(::benchmark::State& state) {
  BM_ReadDoubleColumn_Plain(state, CARD_MEDIUM, Encoding::BYTE_STREAM_SPLIT);
}

static void BM_ReadDoubleColumn_ByteStreamSplit_Wide(::benchmark::State& state) {
  BM_ReadDoubleColumn_Plain(state, CARD_WIDE, Encoding::BYTE_STREAM_SPLIT);
}

BENCHMARK(BM_ReadDoubleColumn_ByteStreamSplit_Narrow)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_ReadDoubleColumn_ByteStreamSplit_Medium)->Args({MAX_SIZE})->UseManualTime();
BENCHMARK(BM_ReadDoubleColumn_ByteStreamSplit_Wide)->Args({MAX_SIZE})->UseManualTime();

/************************ByteArray Plain Tests****************************/
// Plain Encoding
void BM_PlainEncodingByteArray(::benchmark::State& state) {
  auto min_length = static_cast<int32_t>(state.range(0));
  auto max_length = static_cast<int32_t>(state.range(1));
  auto array_size = static_cast<int32_t>(state.range(2));
  auto cardinality = static_cast<int32_t>(state.range(3));
  auto encoder = MakeTypedEncoder<ByteArrayType>(Encoding::PLAIN);
  int64_t bytes_processed = 0;
  auto values = GenerateByteArrayInput(array_size, cardinality, min_length, max_length,
                                       bytes_processed);

  for (auto _ : state) {
    encoder->Put(values.data(), static_cast<int>(values.size()));
    encoder->FlushValues();
  }
  state.SetItemsProcessed(state.iterations() * values.size());
  state.SetBytesProcessed(state.iterations() * bytes_processed);
}

// Parameters: {min_length, max_length, array_size,cardinality}
BENCHMARK(BM_PlainEncodingByteArray)->Args({1, 50, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_PlainEncodingByteArray)->Args({25, 25, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_PlainEncodingByteArray)->Args({20, 30, MAX_SIZE, CARD_WIDE});

void DecodingByteArrayBenchmark(::benchmark::State& state, Encoding::type encoding) {
  int64_t bytes_processed = 0;
  auto min_length = static_cast<int32_t>(state.range(0));
  auto max_length = static_cast<int32_t>(state.range(1));
  auto array_size = static_cast<int32_t>(state.range(2));
  auto cardinality = static_cast<int32_t>(state.range(3));
  std::vector<ByteArray> values = GenerateByteArrayInput(
      array_size, cardinality, min_length, max_length, bytes_processed);
  auto encoder = MakeTypedEncoder<ByteArrayType>(encoding);
  encoder->Put(values.data(), static_cast<int>(values.size()));
  std::shared_ptr<Buffer> buf = encoder->FlushValues();
  std::vector<ByteArray> temp_values;
  temp_values.resize(values.size());
  for (auto _ : state) {
    auto decoder = MakeTypedDecoder<ByteArrayType>(encoding);
    decoder->SetData(static_cast<int>(values.size()), buf->data(),
                     static_cast<int>(buf->size()));
    decoder->Decode(temp_values.data(), static_cast<int>(temp_values.size()));
    ::benchmark::DoNotOptimize(temp_values);
  }
  state.SetItemsProcessed(state.iterations() * values.size());
  state.SetBytesProcessed(state.iterations() * bytes_processed);
}

static void BM_PlainDecodingByteArray(::benchmark::State& state) {
  DecodingByteArrayBenchmark(state, Encoding::PLAIN);
}

// Parameters: {min_length, max_length, array_size,cardinality}
BENCHMARK(BM_PlainDecodingByteArray)->Args({1, 50, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_PlainDecodingByteArray)->Args({25, 25, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_PlainDecodingByteArray)->Args({20, 30, MAX_SIZE, CARD_WIDE});

static void BM_WriteByteArrayColumn_Plain(::benchmark::State& state) {
  auto min_length = static_cast<int32_t>(state.range(0));
  auto max_length = static_cast<int32_t>(state.range(1));
  auto array_size = static_cast<int32_t>(state.range(2));
  auto cardinality = static_cast<int32_t>(state.range(3));
  int64_t bytes_processed = 0;
  std::vector<ByteArray> values = GenerateByteArrayInput(
      array_size, cardinality, min_length, max_length, bytes_processed);
  BM_WriteColumn<ByteArrayWriter, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::PLAIN, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
}

BENCHMARK(BM_WriteByteArrayColumn_Plain)->Args({1, 50, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_WriteByteArrayColumn_Plain)->Args({25, 25, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_WriteByteArrayColumn_Plain)->Args({20, 30, MAX_SIZE, CARD_WIDE});

static void BM_ReadByteArrayColumn_Plain(::benchmark::State& state) {
  auto min_length = static_cast<int32_t>(state.range(0));
  auto max_length = static_cast<int32_t>(state.range(1));
  auto array_size = static_cast<int32_t>(state.range(2));
  auto cardinality = static_cast<int32_t>(state.range(3));
  int64_t bytes_processed = 0;
  std::vector<ByteArray> values = GenerateByteArrayInput(
      array_size, cardinality, min_length, max_length, bytes_processed);
  BM_ReadColumn<ByteArrayWriter, ByteArrayReader, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::PLAIN, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
}

BENCHMARK(BM_ReadByteArrayColumn_Plain)->Args({1, 50, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_ReadByteArrayColumn_Plain)->Args({20, 30, MAX_SIZE, CARD_WIDE});
BENCHMARK(BM_ReadByteArrayColumn_Plain)->Args({25, 25, MAX_SIZE, CARD_WIDE});

/****************ByteArray DeltaLengthEncodingTest********************/

const int DeltaLengthEncodingMinLength = 1;
const int DeltaLengthEncodingMediumLengthLower = 20;
const int DeltaLengthEncodingMediumLengthUpper = 30;
const int DeltaLengthEncodingMaxLength = 50;
void BM_DeltaLengthEncodingByteArray(::benchmark::State& state,
                                     std::vector<ByteArray> values,
                                     int64_t bytes_processed) {
  auto encoder = MakeTypedEncoder<ByteArrayType>(Encoding::DELTA_LENGTH_BYTE_ARRAY);
  for (auto _ : state) {
    encoder->Put(values.data(), static_cast<int>(values.size()));
    encoder->FlushValues();
  }
  state.SetItemsProcessed(state.iterations() * values.size());
  state.SetBytesProcessed(state.iterations() * bytes_processed);
  state.counters["bytes_processed"] = bytes_processed;
}

/**
 * Benchmark for fixed length byte array encoding.
 * The byte array length is fixed to 15.
 */
static void BM_DeltaLengthEncodingByteArray_Fix(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int32_t str_length = (DeltaLengthEncodingMaxLength + DeltaLengthEncodingMinLength) / 2;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length, str_length,
                                       bytes_processed);
  BM_DeltaLengthEncodingByteArray(state, values, bytes_processed);
}

/**
 * Benchmark for byte array encoding with medium level of length variation.
 * The byte array length is randomly generated between 10 and 15.
 */
static void BM_DeltaLengthEncodingByteArray_Medium(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMediumLengthLower;
  int str_length_max = DeltaLengthEncodingMediumLengthUpper;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_DeltaLengthEncodingByteArray(state, values, bytes_processed);
}

/**
 * Benchmark for byte array encoding with high level of length variation.
 * The byte array length is randomly generated between 10 and 1024.
 */
static void BM_DeltaLengthEncodingByteArray_Scatter(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMinLength;
  int str_length_max = DeltaLengthEncodingMaxLength;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_DeltaLengthEncodingByteArray(state, values, bytes_processed);
}

BENCHMARK(BM_DeltaLengthEncodingByteArray_Fix)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthEncodingByteArray_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthEncodingByteArray_Scatter)->Args({MAX_SIZE});

void BM_DeltaLengthDecodingByteArray(::benchmark::State& state,
                                     std::vector<ByteArray> values,
                                     int64_t bytes_processed) {
  auto encoder = MakeTypedEncoder<ByteArrayType>(Encoding::DELTA_LENGTH_BYTE_ARRAY);
  encoder->Put(values.data(), static_cast<int>(values.size()));
  std::shared_ptr<Buffer> buf = encoder->FlushValues();
  for (auto _ : state) {
    auto decoder = MakeTypedDecoder<ByteArrayType>(Encoding::DELTA_LENGTH_BYTE_ARRAY);
    decoder->SetData(static_cast<int>(values.size()), buf->data(),
                     static_cast<int>(buf->size()));
    decoder->Decode(values.data(), static_cast<int>(values.size()));
    ::benchmark::DoNotOptimize(values);
  }
  state.SetItemsProcessed(state.iterations() * values.size());
  state.SetBytesProcessed(state.iterations() * bytes_processed);
  state.counters["bytes_processed"] = bytes_processed;
}

/**
 * Benchmark for fixed length byte array encoding.
 * The byte array length is fixed to 15.
 */
static void BM_DeltaLengthDecodingByteArray_Fix(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int32_t str_length = (DeltaLengthEncodingMaxLength + DeltaLengthEncodingMinLength) / 2;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length, str_length,
                                       bytes_processed);
  BM_DeltaLengthDecodingByteArray(state, values, bytes_processed);
}

/**
 * Benchmark for byte array encoding with medium level of length variation.
 * The byte array length is randomly generated between 10 and 15.
 */
static void BM_DeltaLengthDecodingByteArray_Medium(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMinLength;
  int str_length_max = DeltaLengthEncodingMaxLength;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_DeltaLengthDecodingByteArray(state, values, bytes_processed);
}

/**
 * Benchmark for byte array encoding with high level of length variation.
 * The byte array length is randomly generated between 10 and 1024.
 */
static void BM_DeltaLengthDecodingByteArray_Scatter(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMinLength;
  int str_length_max = DeltaLengthEncodingMaxLength;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_DeltaLengthDecodingByteArray(state, values, bytes_processed);
}

BENCHMARK(BM_DeltaLengthDecodingByteArray_Fix)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthDecodingByteArray_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthDecodingByteArray_Scatter)->Args({MAX_SIZE});

static void BM_DeltaLengthWritingByteArray_Fix(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int32_t str_length = (DeltaLengthEncodingMaxLength + DeltaLengthEncodingMinLength) / 2;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length, str_length,
                                       bytes_processed);
  BM_WriteColumn<ByteArrayWriter, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_LENGTH_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
}

static void BM_DeltaLengthWritingByteArray_Medium(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMediumLengthLower;
  int str_length_max = DeltaLengthEncodingMediumLengthUpper;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_WriteColumn<ByteArrayWriter, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_LENGTH_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
}

static void BM_DeltaLengthWritingByteArray_Scatter(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMinLength;
  int str_length_max = DeltaLengthEncodingMaxLength;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_WriteColumn<ByteArrayWriter, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_LENGTH_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
}

BENCHMARK(BM_DeltaLengthWritingByteArray_Fix)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthWritingByteArray_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthWritingByteArray_Scatter)->Args({MAX_SIZE});

static void BM_DeltaLengthReadingByteArray_Fix(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int32_t str_length = (DeltaLengthEncodingMaxLength + DeltaLengthEncodingMinLength) / 2;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length, str_length,
                                       bytes_processed);
  BM_ReadColumn<ByteArrayWriter, ByteArrayReader, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_LENGTH_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
  state.counters["bytes_processed"] = bytes_processed;
}

static void BM_DeltaLengthReadingByteArray_Medium(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMediumLengthLower;
  int str_length_max = DeltaLengthEncodingMediumLengthUpper;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_ReadColumn<ByteArrayWriter, ByteArrayReader, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_LENGTH_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
  state.counters["bytes_processed"] = bytes_processed;
}

static void BM_DeltaLengthReadingByteArray_Scatter(::benchmark::State& state) {
  int64_t bytes_processed = 0;
  int str_length_min = DeltaLengthEncodingMinLength;
  int str_length_max = DeltaLengthEncodingMaxLength;
  auto values = GenerateByteArrayInput(state.range(0), CARD_WIDE, str_length_min,
                                       str_length_max, bytes_processed);
  BM_ReadColumn<ByteArrayWriter, ByteArrayReader, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_LENGTH_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
  state.counters["bytes_processed"] = bytes_processed;
}

BENCHMARK(BM_DeltaLengthReadingByteArray_Fix)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthReadingByteArray_Medium)->Args({MAX_SIZE});
BENCHMARK(BM_DeltaLengthReadingByteArray_Scatter)->Args({MAX_SIZE});

/**********************ByteArray Dictionary*******************************/
static void BM_DictEncodingByteArray(::benchmark::State& state, int cardinality) {
  int64_t bytes_processed = 0;
  auto min_length = static_cast<int32_t>(state.range(0));
  auto max_length = static_cast<int32_t>(state.range(1));
  auto array_size = static_cast<int32_t>(state.range(2));
  auto values = GenerateByteArrayInput(array_size, cardinality, min_length, max_length,
                                       bytes_processed);
  auto encoder = MakeTypedEncoder<ByteArrayType>(Encoding::PLAIN /*Encoding*/,
                                                 true /*Use Dictionary*/);
  EncodeDict<ByteArrayType>(values, state, ByteArraySchema(Repetition::REQUIRED));
  state.SetItemsProcessed(state.iterations() * values.size());
  state.SetBytesProcessed(state.iterations() * bytes_processed);
  state.counters["bytes_processed"] = bytes_processed;
}
static void BM_DictEncodingByteArray_Narrow(::benchmark::State& state) {
  BM_DictEncodingByteArray(state, CARD_NARROW);
}

static void BM_DictEncodingByteArray_Medium(::benchmark::State& state) {
  BM_DictEncodingByteArray(state, CARD_MEDIUM);
}

static void BM_DictEncodingByteArray_Wide(::benchmark::State& state) {
  BM_DictEncodingByteArray(state, CARD_WIDE);
}

BENCHMARK(BM_DictEncodingByteArray_Narrow)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_DictEncodingByteArray_Medium)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_DictEncodingByteArray_Wide)->Args({0, 200, MAX_SIZE});

static void BM_DictDecodingByteArray(::benchmark::State& state, int cardinality) {
  // Using arrow generator to generate random data.
  int64_t bytes_processed = 0;
  int32_t min_length = static_cast<int32_t>(state.range(0));
  int32_t max_length = static_cast<int32_t>(state.range(1));
  int32_t array_size = static_cast<int32_t>(state.range(2));
  auto values = GenerateByteArrayInput(array_size, cardinality, min_length, max_length,
                                       bytes_processed);
  auto encoder = MakeTypedEncoder<ByteArrayType>(Encoding::PLAIN /*Encoding*/,
                                                 true /*Use Dictionary*/);
  DecodeDict<ByteArrayType>(values, state, ByteArraySchema(Repetition::REQUIRED));
  state.SetItemsProcessed(state.iterations() * values.size());
  state.SetBytesProcessed(state.iterations() * bytes_processed);
}

static void BM_DictDecodingByteArray_Narrow(::benchmark::State& state) {
  BM_DictDecodingByteArray(state, CARD_NARROW);
}

static void BM_DictDecodingByteArray_Medium(::benchmark::State& state) {
  BM_DictDecodingByteArray(state, CARD_MEDIUM);
}

static void BM_DictDecodingByteArray_Wide(::benchmark::State& state) {
  BM_DictDecodingByteArray(state, CARD_WIDE);
}

BENCHMARK(BM_DictDecodingByteArray_Narrow)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_DictDecodingByteArray_Medium)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_DictDecodingByteArray_Wide)->Args({0, 200, MAX_SIZE});

static void BM_WriteByteArrayColumn_Dictionary(::benchmark::State& state,
                                               int cardinality) {
  auto min_length = static_cast<int32_t>(state.range(0));
  auto max_length = static_cast<int32_t>(state.range(1));
  auto array_size = static_cast<int32_t>(state.range(2));
  int64_t bytes_processed = 0;
  std::vector<ByteArray> values = GenerateByteArrayInput(
      array_size, cardinality, min_length, max_length, bytes_processed);
  BM_WriteColumn<ByteArrayWriter, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::PLAIN, true, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
}

static void BM_WriteByteArrayColumn_Dictionary_Narrow(::benchmark::State& state) {
  BM_WriteByteArrayColumn_Dictionary(state, CARD_NARROW);
}

static void BM_WriteByteArrayColumn_Dictionary_Medium(::benchmark::State& state) {
  BM_WriteByteArrayColumn_Dictionary(state, CARD_MEDIUM);
}

static void BM_WriteByteArrayColumn_Dictionary_Wide(::benchmark::State& state) {
  BM_WriteByteArrayColumn_Dictionary(state, CARD_WIDE);
}

BENCHMARK(BM_WriteByteArrayColumn_Dictionary_Narrow)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_WriteByteArrayColumn_Dictionary_Medium)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_WriteByteArrayColumn_Dictionary_Wide)->Args({0, 200, MAX_SIZE});

static void BM_ReadByteArrayColumn_Dictionary(::benchmark::State& state,
                                              int cardinality) {
  auto min_length = static_cast<int32_t>(state.range(0));
  auto max_length = static_cast<int32_t>(state.range(1));
  auto array_size = static_cast<int32_t>(state.range(2));
  int64_t bytes_processed = 0;
  std::vector<ByteArray> values = GenerateByteArrayInput(
      array_size, cardinality, min_length, max_length, bytes_processed);
  BM_ReadColumn<ByteArrayWriter, ByteArrayReader, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::PLAIN, true, values,
      ByteArraySchema(Repetition::REQUIRED), bytes_processed);
}

static void BM_ReadByteArrayColumn_Dictionary_Narrow(::benchmark::State& state) {
  BM_ReadByteArrayColumn_Dictionary(state, CARD_NARROW);
}

static void BM_ReadByteArrayColumn_Dictionary_Medium(::benchmark::State& state) {
  BM_ReadByteArrayColumn_Dictionary(state, CARD_MEDIUM);
}

static void BM_ReadByteArrayColumn_Dictionary_Wide(::benchmark::State& state) {
  BM_ReadByteArrayColumn_Dictionary(state, CARD_WIDE);
}

BENCHMARK(BM_ReadByteArrayColumn_Dictionary_Narrow)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_ReadByteArrayColumn_Dictionary_Medium)->Args({0, 200, MAX_SIZE});
BENCHMARK(BM_ReadByteArrayColumn_Dictionary_Wide)->Args({0, 200, MAX_SIZE});

/**********************ByteArray DeltaEncoding******************************/
struct DeltaByteArrayState {
  int32_t min_size = 0;
  int32_t max_size;
  int32_t array_length;
  int32_t total_data_size = 0;
  double prefixed_probability;
  std::vector<uint8_t> buf;

  explicit DeltaByteArrayState(const ::benchmark::State& state)
      : max_size(static_cast<int32_t>(state.range(0))),
        array_length(static_cast<int32_t>(state.range(1))),
        prefixed_probability(state.range(2) / 100.0) {}

  std::vector<ByteArray> MakeRandomByteArray(uint32_t seed) {
    std::default_random_engine gen(seed);
    std::uniform_int_distribution<int> dist_size(min_size, max_size);
    std::uniform_int_distribution<int> dist_byte(0, 255);
    std::bernoulli_distribution dist_has_prefix(prefixed_probability);
    std::uniform_real_distribution<double> dist_prefix_length(0, 1);

    std::vector<ByteArray> out(array_length);
    buf.resize(max_size * array_length);
    auto buf_ptr = buf.data();
    total_data_size = 0;

    for (int32_t i = 0; i < array_length; ++i) {
      int len = dist_size(gen);
      out[i].len = len;
      out[i].ptr = buf_ptr;

      bool do_prefix = i > 0 && dist_has_prefix(gen);
      int prefix_len = 0;
      if (do_prefix) {
        int max_prefix_len = std::min(len, static_cast<int>(out[i - 1].len));
        prefix_len =
            static_cast<int>(std::ceil(max_prefix_len * dist_prefix_length(gen)));
      }
      for (int j = 0; j < prefix_len; ++j) {
        buf_ptr[j] = out[i - 1].ptr[j];
      }
      for (int j = prefix_len; j < len; ++j) {
        buf_ptr[j] = static_cast<uint8_t>(dist_byte(gen));
      }
      buf_ptr += len;
      total_data_size += len;
    }
    return out;
  }
};

static void BM_DeltaEncodingByteArray(::benchmark::State& state) {
  DeltaByteArrayState delta_state(state);
  std::vector<ByteArray> values = delta_state.MakeRandomByteArray(/*seed=*/42);

  auto encoder = MakeTypedEncoder<ByteArrayType>(Encoding::DELTA_BYTE_ARRAY);
  const int64_t plain_encoded_size =
      delta_state.total_data_size + 4 * delta_state.array_length;
  int64_t encoded_size = 0;

  for (auto _ : state) {
    encoder->Put(values.data(), static_cast<int>(values.size()));
    encoded_size = encoder->FlushValues()->size();
  }
  state.SetItemsProcessed(state.iterations() * delta_state.array_length);
  state.SetBytesProcessed(state.iterations() * delta_state.total_data_size);
  state.counters["compression_ratio"] =
      static_cast<double>(plain_encoded_size) / encoded_size;
}

static void BM_DeltaDecodingByteArray(::benchmark::State& state) {
  DeltaByteArrayState delta_state(state);
  std::vector<ByteArray> values = delta_state.MakeRandomByteArray(/*seed=*/42);

  auto encoder = MakeTypedEncoder<ByteArrayType>(Encoding::DELTA_BYTE_ARRAY);
  encoder->Put(values.data(), static_cast<int>(values.size()));
  std::shared_ptr<Buffer> buf = encoder->FlushValues();

  const int64_t plain_encoded_size =
      delta_state.total_data_size + 4 * delta_state.array_length;
  const int64_t encoded_size = buf->size();

  auto decoder = MakeTypedDecoder<ByteArrayType>(Encoding::DELTA_BYTE_ARRAY);
  for (auto _ : state) {
    decoder->SetData(delta_state.array_length, buf->data(),
                     static_cast<int>(buf->size()));
    decoder->Decode(values.data(), static_cast<int>(values.size()));
    ::benchmark::DoNotOptimize(values);
  }
  state.SetItemsProcessed(state.iterations() * delta_state.array_length);
  state.SetBytesProcessed(state.iterations() * delta_state.total_data_size);
  state.counters["compression_ratio"] =
      static_cast<double>(plain_encoded_size) / encoded_size;
}

static void ByteArrayDeltaCustomArguments(::benchmark::internal::Benchmark* b) {
  int max_string_length = 50;
  int batch_size = 65536;
  for (int prefixed_percent : {10, 50, 99}) {
    b->Args({max_string_length, batch_size, prefixed_percent});
  }
  b->ArgNames({"max-string-length", "batch-size", "prefixed-percent"});
}

BENCHMARK(BM_DeltaEncodingByteArray)->Apply(ByteArrayDeltaCustomArguments);
BENCHMARK(BM_DeltaDecodingByteArray)->Apply(ByteArrayDeltaCustomArguments);

static void BM_WriteByteArrayColumn_Delta(::benchmark::State& state) {
  DeltaByteArrayState delta_state(state);
  std::vector<ByteArray> values = delta_state.MakeRandomByteArray(/*seed=*/42);

  BM_WriteColumn<ByteArrayWriter, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), delta_state.total_data_size);
}

static void BM_ReadByteArrayColumn_Delta(::benchmark::State& state) {
  DeltaByteArrayState delta_state(state);
  std::vector<ByteArray> values = delta_state.MakeRandomByteArray(/*seed=*/42);

  BM_ReadColumn<ByteArrayWriter, ByteArrayReader, ByteArrayType>(
      state, Compression::UNCOMPRESSED, Encoding::DELTA_BYTE_ARRAY, false, values,
      ByteArraySchema(Repetition::REQUIRED), delta_state.total_data_size);
}

BENCHMARK(BM_WriteByteArrayColumn_Delta)->Apply(ByteArrayDeltaCustomArguments);
BENCHMARK(BM_ReadByteArrayColumn_Delta)->Apply(ByteArrayDeltaCustomArguments);

// Arrow Reading Writing Test
static void BM_ArrowWrite(::benchmark::State& state) {
  auto input_values = GenerateSeqInt64(state.range(0), 0);
  for (auto _ : state) {
    // Clear the filesystem cache (requires root access)
    // auto code = system("echo 1 | sudo tee /proc/sys/vm/drop_caches > /dev/null");

    auto drop_cache = system(
        "dd of=/tmp/bm_tmp.parquet oflag=nocache conv=notrunc,fdatasync status=none "
        "count=0");
    // Create a file output stream
    std::shared_ptr<::arrow::io::FileOutputStream> stream;
    PARQUET_ASSIGN_OR_THROW(stream, ::arrow::io::FileOutputStream::Open(
                                        tmp_file_path, false /*ie.: Truncate*/));

    // Start timing
    auto start = std::chrono::high_resolution_clock::now();
    stream->Write(reinterpret_cast<const uint8_t*>(input_values.data()),
                  input_values.size() * sizeof(int64_t));
    stream->Close();
    auto end = std::chrono::high_resolution_clock::now();
    auto elapsed_seconds =
        std::chrono::duration_cast<std::chrono::duration<double>>(end - start);
    state.SetIterationTime(elapsed_seconds.count());
  }
  state.SetBytesProcessed(state.iterations() * input_values.size() * sizeof(int64_t));
}
BENCHMARK(BM_ArrowWrite)->Range(MIN_SIZE, MAX_SIZE)->UseManualTime();

static void BM_ArrowRead(::benchmark::State& state) {
  // Writing values in
  auto input_values = GenerateSeqInt64(state.range(0), 0);
  std::shared_ptr<::arrow::io::FileOutputStream> stream;
  PARQUET_ASSIGN_OR_THROW(stream, ::arrow::io::FileOutputStream::Open(
                                      tmp_file_path, false /*ie.: Truncate*/));
  stream->Write(reinterpret_cast<const uint8_t*>(input_values.data()),
                input_values.size() * sizeof(int64_t));
  stream->Close();
  for (auto _ : state) {
    // Clear the filesystem cache (requires root access)
    // auto code = system("echo 1 | sudo tee /proc/sys/vm/drop_caches > /dev/null");

    auto drop_cache = system(
        "dd of=/tmp/bm_tmp.parquet oflag=nocache conv=notrunc,fdatasync status=none "
        "count=0");
    // Create a file input stream
    std::shared_ptr<::arrow::io::ReadableFile> raw_src_file;
    PARQUET_ASSIGN_OR_THROW(raw_src_file, ::arrow::io::ReadableFile::Open(tmp_file_path));
    // Determine the size of the file
    int64_t file_size = 0;
    PARQUET_ASSIGN_OR_THROW(file_size, raw_src_file->GetSize());
    std::shared_ptr<::arrow::Buffer> buffer;
    std::vector<int64_t> vector_buffer;
    vector_buffer.resize(input_values.size());
    // Start timing
    auto start = std::chrono::high_resolution_clock::now();
    // PARQUET_ASSIGN_OR_THROW(buffer, raw_src_file->Read(file_size));
    raw_src_file->Read(file_size, vector_buffer.data());
    auto end = std::chrono::high_resolution_clock::now();
    auto real_reading_size = buffer->size();
    raw_src_file->Close();
    auto elapsed_seconds =
        std::chrono::duration_cast<std::chrono::duration<double>>(end - start);
    state.SetIterationTime(elapsed_seconds.count());
  }
  state.SetBytesProcessed(state.iterations() * input_values.size() * sizeof(int64_t));
}

BENCHMARK(BM_ArrowRead)->Range(MIN_SIZE, MAX_SIZE)->UseManualTime();
}  // namespace parquet
