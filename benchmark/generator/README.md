# Parquet Generator
To generate a parquet file, one should first create a json configuration file describing column schema, and use the following
command to generate the file. One may refer `example_config.json` for more details.

```
parquet_generator -j <path_to_json_file> -o <output_path>
``` 
Generating parquet file is divided into two layers, logical data generation and physical data generation. Content of the configuration file will be parsed into a list of `ColumnMeta` objects, describing the properties of each column. Logical generator will take this metadata and produce data. The logic of data production is encapsulated in `data_generator.py`. After this, logical data will be written into a parquet file using `pyarrow.parquet.ParquetWriter`, using per-column compression and encoding configuration declared in `ColumnMeta`.

# Configurable parameters
To support flexible parquet file generation, logical data generator is designed to support various configurations, to mimic various parquet layouts. The table below shows the configurable parameters, note that these parameter names are the same for json configuration file. Attributes marked with `*` is required.

<center>

| Configuration | Description | Accepted Values | Default Value |
| --- | --- | --- | --- |
| *name | Name of the column | String | None, this field is required |
| *dtype | Type of the column | ['int','double','str','struct'] | None, this field is required|
| distribution | value distribution of the column, can be further configured using dist_param configuration | ['normal','uniform','unique','gamma','random'] | 'normal' |
| str_length | The length range used to generate string | None or list of two e.g. [3,4] | None, this field can be empty. If dtype is 'str', a default value [5,10] will be used. (Boundary inclusive) |
| custom_generator | A customized data generator, when this field is not empty, data for this field will be generated by this generator instead of default distribution generator | A functor | None, this field can be empty |
| num_null | Number of null values in the column | Non-negative integer | 0 |
| cardinality | Number of unique values in the column, NULL is not counted. For example if the dataset is [3,3,Null,4] then cardinality is 2  | Non-negative integer | None, this field can be empty. If this field is not set, a default cardinality will be 10% of dataset size|
| num_repeated | Number of allowed values to be repeated, this field is useful to generate `repeated` fields, when the value is 1 it means `required` field. | Integer larger or equal to 1 | 1 |
| dist_params | Extra configurations for distribution | dictionary | None, this field can be empty |
| compression | Compression method for this column | ['NONE', 'SNAPPY', 'GZIP', 'BROTLI', 'LZ4'] | 'NONE' |
| encoding | Encoding algorithm for this column | ['PLAIN', 'BYTE_STREAM_SPLIT', 'DELTA_BINARY_PACKED', 'DELTA_LENGTH_BYTE_ARRAY', 'DELTA_BYTE_ARRAY'] | 'PLAIN' |

</center>

**More details on dist_params**: Though it is only required to declare `dtype` and `name` for a column, it is highly recommended to use `dist_params` as well. Parameters relating to specific distributions can be defined here.

<center>

| Distribution Name | Supported configuration | Default Value |
| --- | --- | --- |
| 'normal' | std: Describes how scatter this distribution should be | 1 |
| 'gamma'  | shape: Affects the distribution's skewness | 2.0 |
| 'gamma'  | scale: The scale parameter modifies the distribution's spread | 1.0 |

</center>

Meanwhile, one can also define the range of numeric values here, `min` and `max` in `dist_params` describes the value range for the distribution.Note that these two fields are only in effect when the data type is "int" or "double" **It is highly recommended to define these two fields as the default setting is in range INT32, which may sometimes crash the process with large scale, high cardinality setting.**


**More details on encoding** 
Note that each encoding method has type limitations, except for `PLAIN` encoding, which can be applied on all types. Make sure the encoding can be applied to specific data type when writing configuration file.

<center>

| Data type | Supported Encoding |
| --- | --- |
| int | ["PLAIN", "DELTA_BINARY_PACKED", "PLAIN_DICTIONARY"] |
| double | ["PLAIN", "BYTE_STREAM_SPLIT", "PLAIN_DICTIONARY"]|
| str | ["PLAIN","DELTA_BYTE_ARRAY","DELTA_LENGTH_BYTE_ARRAY","PLAIN_DICTIONARY"] |

</center>

**More details on distribution**
Note that the semantics of distribution only applies to the values that are not `NULL`. For example, given a column definition with size 1000, if the distribution is normal and the number of null values is set to be 100, then the 900 values will form a normal distribution. 

# Json Configuration File

Configuration json file should have two attributes: `size` and `columns`. `size` is an integer describing the number of rows for each column and `columns` is a list of column definitions. An example would be:

```
{
    "size": 100,
    "columns" : [...]
}

```

Each entity in the `columns` list should define the details of a column, using the configuration parameters discussed above, like the following:

```
    "size": 100,
    "columns": [
        {
            "name": "Int_Column",
            "dtype": "int"/"double"/"str",
            "distribution": "normal"/"uniform"... ,
            "str_length": [3,4],   # Use for string only
            "num_null": 40,
            "cardinality" : 10,
            "dist_params" ={
                "std": 2,
                "min": 10,
                "max": 1000,
            },
            "compression": "NONE"/"GZIP".. ,
            "encoding": "PLAIN"/"PLAIN_DICTIONARY"..
        },
        {
            column2...
        }
    ]
```